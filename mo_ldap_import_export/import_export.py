# SPDX-FileCopyrightText: 2019-2020 Magenta ApS
# SPDX-License-Identifier: MPL-2.0
import asyncio
from collections import ChainMap
from collections.abc import Awaitable
from collections.abc import Callable
from collections.abc import Generator
from collections.abc import Iterator
from collections.abc import MutableMapping
from collections.abc import Sequence
from contextlib import ExitStack
from functools import wraps
from typing import Any
from typing import Protocol
from typing import TypeVar
from typing import cast
from uuid import UUID
from uuid import uuid4

import structlog
from fastapi.encoders import jsonable_encoder
from fastramqpi.ramqp.depends import handle_exclusively_decorator
from fastramqpi.ramqp.utils import RequeueMessage
from ldap3 import Connection
from more_itertools import all_equal
from more_itertools import first
from more_itertools import one
from more_itertools import only
from more_itertools import partition
from more_itertools import quantify
from ramodels.mo import MOBase
from ramodels.mo.details.address import Address
from ramodels.mo.details.engagement import Engagement
from ramodels.mo.details.it_system import ITUser
from ramodels.mo.employee import Employee
from structlog.contextvars import bound_contextvars

from .autogenerated_graphql_client import GraphQLClient
from .autogenerated_graphql_client.input_types import AddressFilter
from .autogenerated_graphql_client.input_types import ClassFilter
from .autogenerated_graphql_client.input_types import EmployeeFilter
from .autogenerated_graphql_client.input_types import EngagementFilter
from .autogenerated_graphql_client.input_types import ITSystemFilter
from .autogenerated_graphql_client.input_types import ITUserFilter
from .autogenerated_graphql_client.input_types import OrganisationUnitFilter
from .config import Settings
from .converters import LdapConverter
from .customer_specific_checks import ExportChecks
from .customer_specific_checks import ImportChecks
from .dataloaders import DN
from .dataloaders import DataLoader
from .dataloaders import Verb
from .dataloaders import extract_current_or_latest_validity
from .exceptions import DNNotFound
from .ldap import apply_discriminator
from .ldap import get_ldap_object
from .ldap_classes import LdapObject
from .types import EmployeeUUID
from .types import OrgUnitUUID
from .utils import bucketdict
from .utils import extract_ou_from_dn
from .utils import get_delete_flag
from .utils import star

logger = structlog.stdlib.get_logger()


T = TypeVar("T", covariant=True)


class HasValidities(Protocol[T]):
    @property
    def validities(self) -> Sequence[T]:  # pragma: no cover
        ...


class HasObjects(Protocol[T]):
    @property
    def objects(self) -> Sequence[T]:  # pragma: no cover
        ...


def flatten_validities(
    response: HasObjects[HasValidities[T]],
) -> Generator[T, None, None]:
    for obj in response.objects:
        yield from obj.validities


async def get_primary_engagement(
    graphql_client: GraphQLClient, uuid: EmployeeUUID
) -> UUID | None:
    """Decide the best primary engagement for the provided user.

    Args:
        uuid: UUID of the user to find the primary engagement for.

    Raises:
        RequeueMessage: If the method wants to wait for calculate_primary to run.

    Returns:
        The UUID of an engagement if found, otherwise None.
    """
    # TODO: Implement suppport for selecting primary engagements directly from MO
    # Get engagements from MO
    result = await graphql_client.read_engagements_is_primary(
        EngagementFilter(
            employee=EmployeeFilter(uuids=[uuid]), from_date=None, to_date=None
        )
    )
    # Flatten all validities to a list
    validities = list(flatten_validities(result))
    # No validities --> no primary
    if not validities:
        logger.info("No engagement validities found")
        return None

    # Remove all non-primary validities
    # This should contain a list of non-overlapping primary engagement validities,
    # assuming that primary calculation has run succesfully, overlaps indicate that
    # calculate_primary has not done its job correctly.
    # TODO: Check this invariant and throw RequeueMessage whenever it is broken?
    primary_validities = [val for val in validities if val.is_primary]

    # If there is validities, but none of them are primary, we need to wait for
    # calculate_primary to determine which validities are supposed to be primary.
    # TODO: Consider if we actually care to wait, we could just return `None` and
    #       notify that there is no primary while waiting for another AMQP message
    #       to come in, whenever calculate_primary has made changes.
    #       This however requires the engagement listener to actually trigger all
    #       code-paths that may end up calling this function.
    #       So for now we play it safe and keep this AMQP event around by requeuing.
    if validities and not primary_validities:
        logger.info(
            "Waiting for primary engagement to be decided",
            validities=validities,
            primary_validities=[],
        )
        raise RequeueMessage("Waiting for primary engagement to be decided")

    try:
        primary_engagement_validity = extract_current_or_latest_validity(
            primary_validities
        )
    except ValueError as e:
        # Multiple current primary engagements found, we cannot handle this
        # situation gracefully, so we requeue until calculate_primary resolves it.
        # NOTE: There may in fact still be multiple primary engagements in the past
        #       or future, but these are resolved by simply picking the latest one.
        # TODO: This should probably be fixed so we detect all overlaps
        logger.warning(
            "Waiting for multiple primary engagements to be resolved",
            validities=validities,
            primary_validities=primary_validities,
        )
        raise RequeueMessage(
            "Waiting for multiple primary engagements to be resolved"
        ) from e

    # No primary engagement identified, not even a delete/past ones
    # This should never occur since we check for primary_validities before calling
    # the extract_current_or_latest_object function. See the TODO for this check.
    # TODO: If we end up removing that check, then we should probably log and
    #       return None here instead of asserting it never happens.
    assert primary_engagement_validity is not None

    primary_engagement_uuid = primary_engagement_validity.uuid

    logger.info(
        "Found primary engagement",
        validities=validities,
        primary_validities=primary_validities,
        primary_engagement_uuid=primary_engagement_uuid,
    )
    return primary_engagement_uuid


def with_exitstack(
    func: Callable[..., Awaitable[T]],
) -> Callable[..., Awaitable[T]]:
    """Inject an exit-stack into decorated function.

    Args:
        func: The function to inject the exit-stack into.

    Returns:
        Decorated function that takes the exit-stack as an argument.
    """

    @wraps(func)
    async def inner(*args: Any, **kwargs: Any) -> Any:
        with ExitStack() as exit_stack:
            return await func(*args, **kwargs, exit_stack=exit_stack)

    return inner


class SyncTool:
    def __init__(
        self,
        dataloader: DataLoader,
        converter: LdapConverter,
        export_checks: ExportChecks,
        import_checks: ImportChecks,
        settings: Settings,
        ldap_connection: Connection,
    ) -> None:
        self.dataloader: DataLoader = dataloader
        self.converter: LdapConverter = converter
        self.export_checks: ExportChecks = export_checks
        self.import_checks: ImportChecks = import_checks
        self.settings: Settings = settings
        self.ldap_connection: Connection = ldap_connection

    @staticmethod
    def wait_for_import_to_finish(func: Callable):
        """Runs the function while ensuring sequentiality w.r.t. the dn parameter."""

        def dn_extractor(self, *args, **kwargs):
            dn = args[0] if args else kwargs["dn"]
            logger.info("Generating DN", dn=dn)
            return dn

        return handle_exclusively_decorator(dn_extractor)(func)

    async def perform_export_checks(self, employee_uuid: UUID, object_uuid: UUID):
        """
        Perform a number of customer-specific checks. Raising IgnoreChanges() if a
        check fails
        """

        # Check that the employee has an it-user with user_key = it_user_to_check
        await self.export_checks.check_it_user(
            employee_uuid,
            self.settings.it_user_to_check,
        )

    async def perform_import_checks(self, dn: str, json_key: str) -> bool:
        if self.settings.check_holstebro_ou_issue_57426:
            return await self.import_checks.check_holstebro_ou_is_externals_issue_57426(
                self.settings.check_holstebro_ou_issue_57426,
                dn,
                json_key,
            )
        return True

    async def move_ldap_object(self, ldap_object: LdapObject, dn: DN) -> LdapObject:
        """
        Parameters
        ----------------
        ldap_object: LdapObject
            LDAP object as converted by converter.to_ldap()
        dn: str
            DN which we expect the object to have

        Notes
        -----------
        If the DN on the ldap object is different from the supplied dn, we move the
        object in LDAP, so the two match. We always assume that the DN on the LDAP
        object is correct, because that one is mapped in the json file.
        """
        old_dn = dn
        new_dn = ldap_object.dn

        if new_dn == old_dn:
            return ldap_object

        old_ou = extract_ou_from_dn(old_dn)
        new_ou = extract_ou_from_dn(new_dn)

        logger.info(
            "Moving user to new organizational unit",
            old_ou=old_ou,
            new_ou=new_ou,
            old_dn=old_dn,
            new_dn=new_dn,
        )

        # Create the new OU (dataloader.create_ou checks if it exists)
        await self.dataloader.ldapapi.create_ou(new_ou)

        # Move the object to the proper OU
        move_successful = await self.dataloader.move_ldap_object(old_dn, new_dn)

        if move_successful:
            # Delete the old OU (dataloader.delete_ou checks if it is empty)
            await self.dataloader.delete_ou(old_ou)
        else:
            ldap_object.dn = old_dn

        return ldap_object

    async def mo_person_to_ldap(
        self,
        uuid: EmployeeUUID,
        dn: DN,
        mo_object_dict: MutableMapping[str, Any],
    ) -> LdapObject:
        """Synchronize employee person from MO to LDAP.

        Args:
            uuid: UUID of the employee to synchronize from.
            dn: DN of the LDAP account to synchronize to.
            mo_object_dict: Template context for mapping templates.
        """
        await self.perform_export_checks(uuid, uuid)

        # Convert to LDAP
        ldap_employee = await self.converter.to_ldap(mo_object_dict, "Employee", dn)
        return ldap_employee

    async def mo_address_to_ldap(
        self,
        uuid: EmployeeUUID,
        dn: DN,
        mo_object_dict: MutableMapping[str, Any],
    ) -> dict[str, tuple[LdapObject, bool]]:
        """Synchronize employee addresses from MO to LDAP.

        Args:
            uuid: UUID of the employee to synchronize from.
            dn: DN of the LDAP account to synchronize to.
            mo_object_dict: Template context for mapping templates.
        """

        # When mapping addresses, the key is the user-key of the address type
        mapped_address_types = {
            key
            for key, mapping in self.settings.conversion_mapping.ldap_to_mo.items()
            if mapping.objectClass == "ramodels.mo.details.address.Address"
            and not hasattr(mapping, "org_unit")
        }
        if not mapped_address_types:
            return {}
        # Get MO addresses
        result = await self.dataloader.graphql_client.read_filtered_addresses(
            AddressFilter(
                employee=EmployeeFilter(uuids=[uuid]),
                address_type=ClassFilter(user_keys=mapped_address_types),
                from_date=None,
                to_date=None,
            )
        )
        address_validities = [
            extract_current_or_latest_validity(obj.validities) for obj in result.objects
        ]
        addresses = [obj for obj in address_validities if obj is not None]
        # Group addresses by address-type
        address_map = bucketdict(
            addresses, key=lambda address: address.address_type.user_key
        )
        # TODO: Support deletion here, possibly by detecting address_types in our
        #       configuration, which are not in the address_map keys, i.e.
        #       mapped_address_types - address_map.keys()
        #       Then again, deletions are unlikely to work fully without knowing the
        #       entire state that has to be transferred, so this is probably the issue
        #       that should be resolved instead.
        # TODO: Consider partitioning by the above set overlap into creates, modify
        #       and deletes directly.
        result_map = {}
        for address_type, addresses in address_map.items():
            # At most one address of each type should exist, as we only map one
            # If more exists, our program either sucks or someone else did it
            # TODO: In the future we want to panic in this case with RequeueMessage
            #       However for now, we are afraid of breaking the program too much
            if len(addresses) > 1:
                logger.warning(
                    "Multiple addresses of same type",
                    address_type=address_type,
                    employee_uuid=uuid,
                )
            # This is a horrible solution which just synchronizes a determinitic
            # but otherwise arbitrary address, it is however better than a before
            # where we synchronized a non-deterministic random address
            #
            # TODO: replace the line below, with the following block, if the warning
            #       above is never triggered.
            # too_short = too_long = RequeueMessage(
            #    f"More than one address with type: {address_type}"
            # )
            # changed_address = one(addresses, too_short=too_short, too_long=too_long)
            changed_address = min(addresses, key=lambda address: address.uuid)

            await self.perform_export_checks(uuid, changed_address.uuid)

            # TODO: Fetch the required fields directly during `read_filtered_addresses`
            fetched_address = await self.dataloader.load_mo_address(
                changed_address.uuid, current_objects_only=False
            )
            if fetched_address is None:
                logger.error("Unable to load mo address")
                raise RequeueMessage("Unable to load mo address")
            delete = get_delete_flag(jsonable_encoder(fetched_address))

            template_dict = ChainMap(
                {"mo_employee_address": fetched_address}, mo_object_dict
            )

            logger.info(
                "Obtained address", address_type=address_type, uuid=changed_address.uuid
            )
            # Convert & Upload to LDAP
            ldap_object = await self.converter.to_ldap(template_dict, address_type, dn)
            result_map[address_type] = (ldap_object, delete)
        return result_map

    async def mo_org_unit_address_to_ldap(
        self,
        uuid: EmployeeUUID,
        dn: DN,
        mo_object_dict: MutableMapping[str, Any],
    ) -> dict[str, tuple[LdapObject, bool]]:
        """Synchronize org-unit addresses from MO to LDAP.

        Args:
            uuid: UUID of the employee to synchronize from.
            dn: DN of the LDAP account to synchronize to.
            mo_object_dict: Template context for mapping templates.
        """
        # NOTE: This function shares a lot of code with `mo_address_to_ldap`

        primary_engagement_uuid = await get_primary_engagement(
            self.dataloader.graphql_client, uuid
        )
        if primary_engagement_uuid is None:
            return {}

        # When mapping addresses, the key is the user-key of the address type
        mapped_address_types = {
            key
            for key, mapping in self.settings.conversion_mapping.ldap_to_mo.items()
            if mapping.objectClass == "ramodels.mo.details.address.Address"
            and hasattr(mapping, "org_unit")
        }
        if not mapped_address_types:
            return {}
        # Get MO addresses
        result = await self.dataloader.graphql_client.read_filtered_addresses(
            AddressFilter(
                # TODO: Use primary engagement filter here
                org_unit=OrganisationUnitFilter(
                    engagement=EngagementFilter(uuids=[primary_engagement_uuid])
                ),
                address_type=ClassFilter(user_keys=mapped_address_types),
                from_date=None,
                to_date=None,
            )
        )
        address_validities = [
            extract_current_or_latest_validity(obj.validities) for obj in result.objects
        ]
        addresses = [obj for obj in address_validities if obj is not None]
        # Group addresses by address-type
        address_map = bucketdict(
            addresses, key=lambda address: address.address_type.user_key
        )
        # TODO: Support deletion here, possibly by detecting address_types in our
        #       configuration, which are not in the address_map keys, i.e.
        #       mapped_address_types - address_map.keys()
        #       Then again, deletions are unlikely to work fully without knowing the
        #       entire state that has to be transferred, so this is probably the issue
        #       that should be resolved instead.
        # TODO: Consider partitioning by the above set overlap into creates, modify
        #       and deletes directly.
        result_map = {}
        for address_type, addresses in address_map.items():
            # TODO: Not entirely sure why this is an invariant, it is merely retained
            # TODO: Check this statically, as it does not depend on data at all
            #       Potentially during settings parsing?
            ldap_object_class = self.converter.find_ldap_object_class(address_type)
            employee_object_class = self.converter.find_ldap_object_class("Employee")
            if ldap_object_class != employee_object_class:  # pragma: no cover
                logger.warning(
                    "Mapping organization unit addresses "
                    "to non-employee objects is not supported"
                )
                continue

            # At most one address of each type should exist, as we only map one
            # If more exists, our program either sucks or someone else did it
            # TODO: In the future we want to panic in this case with RequeueMessage
            #       However for now, we are afraid of breaking the program too much
            if len(addresses) > 1:
                logger.warning(
                    "Multiple addresses of same type",
                    address_type=address_type,
                    employee_uuid=uuid,
                )
            # This is a horrible solution which just synchronizes a determinitic
            # but otherwise arbitrary address, it is however better than a before
            # where we synchronized a non-deterministic random address
            #
            # TODO: replace the line below, with the following block, if the warning
            #       above is never triggered.
            # too_short = too_long = RequeueMessage(
            #    f"More than one address with type: {address_type}"
            # )
            # changed_address = one(addresses, too_short=too_short, too_long=too_long)
            changed_address = min(addresses, key=lambda address: address.uuid)

            await self.perform_export_checks(uuid, changed_address.uuid)

            # TODO: Fetch the required fields directly during `read_filtered_addresses`
            fetched_address = await self.dataloader.load_mo_address(
                changed_address.uuid, current_objects_only=False
            )
            if fetched_address is None:
                logger.error("Unable to load mo address")
                raise RequeueMessage("Unable to load mo address")
            delete = get_delete_flag(jsonable_encoder(fetched_address))

            template_dict = ChainMap(
                {"mo_org_unit_address": fetched_address}, mo_object_dict
            )

            logger.info(
                "Obtained address", address_type=address_type, uuid=changed_address.uuid
            )
            # Convert & Upload to LDAP
            ldap_object = await self.converter.to_ldap(template_dict, address_type, dn)
            result_map[address_type] = (ldap_object, delete)
        return result_map

    async def mo_ituser_to_ldap(
        self,
        uuid: EmployeeUUID,
        dn: DN,
        mo_object_dict: MutableMapping[str, Any],
    ) -> dict[str, tuple[LdapObject, bool]]:
        """Synchronize employee itusers from MO to LDAP.

        Args:
            uuid: UUID of the employee to synchronize from.
            dn: DN of the LDAP account to synchronize to.
            mo_object_dict: Template context for mapping templates.
        """
        # When mapping itusers, the key is the user-key of the itsystem
        mapped_itsystems = {
            key
            for key, mapping in self.settings.conversion_mapping.ldap_to_mo.items()
            if mapping.objectClass == "ramodels.mo.details.it_system.ITUser"
        }
        if not mapped_itsystems:
            return {}
        # Get MO ITUsers
        result = await self.dataloader.graphql_client.read_filtered_itusers(
            ITUserFilter(
                employee=EmployeeFilter(uuids=[uuid]),
                itsystem=ITSystemFilter(user_keys=mapped_itsystems),
                from_date=None,
                to_date=None,
            )
        )
        ituser_validities = [
            extract_current_or_latest_validity(obj.validities) for obj in result.objects
        ]
        itusers = [obj for obj in ituser_validities if obj is not None]
        # Group addresses by address-type
        ituser_map = bucketdict(itusers, key=lambda ituser: ituser.itsystem.user_key)
        # TODO: Support deletion here, possibly by detecting itsystems in our
        #       configuration, which are not in the ituser_map keys, i.e.
        #       mapped_itsystems - ituser_map.keys()
        #       Then again, deletions are unlikely to work fully without knowing the
        #       entire state that has to be transferred, so this is probably the issue
        #       that should be resolved instead.
        # TODO: Consider partitioning by the above set overlap into creates, modify
        #       and deletes directly.
        result_map = {}
        for itsystem, itusers in ituser_map.items():
            # At most one ituser of each type should exist, as we only map one
            # If more exists, our program either sucks or someone else did it
            # TODO: In the future we want to panic in this case with RequeueMessage
            #       However for now, we are afraid of breaking the program too much
            if len(itusers) > 1:
                logger.warning(
                    "Multiple itusers with the same itsystem",
                    itsystem=itsystem,
                    employee_uuid=uuid,
                )
            # This is a horrible solution which just synchronizes a determinitic
            # but otherwise arbitrary ituser, it is however better than a before
            # where we synchronized a non-deterministic random ituser
            #
            # TODO: replace the line below, with the following block, if the warning
            #       above is never triggered.
            # too_short = too_long = RequeueMessage(
            #    f"More than one ituser with itsystem: {itsystem}"
            # )
            # changed_address = one(itusers, too_long=too_long)
            changed_ituser = min(itusers, key=lambda ituser: ituser.uuid)

            await self.perform_export_checks(uuid, changed_ituser.uuid)

            # TODO: Fetch the required fields directly during `read_filtered_itusers`
            fetched_ituser = await self.dataloader.load_mo_it_user(
                changed_ituser.uuid, current_objects_only=False
            )
            if fetched_ituser is None:
                logger.error("Unable to load mo it-user")
                raise RequeueMessage("Unable to load mo it-user")
            delete = get_delete_flag(jsonable_encoder(fetched_ituser))

            template_dict = ChainMap(
                {"mo_employee_it_user": fetched_ituser}, mo_object_dict
            )

            logger.info("Obtained ituser", itsystem=itsystem, uuid=changed_ituser.uuid)
            # Convert & Upload to LDAP
            ldap_object = await self.converter.to_ldap(template_dict, itsystem, dn)
            result_map[itsystem] = (ldap_object, delete)
        return result_map

    async def mo_engagement_to_ldap(
        self,
        uuid: EmployeeUUID,
        dn: DN,
        mo_object_dict: dict[str, Any],
    ) -> dict[str, tuple[LdapObject, bool]]:
        primary_engagement_uuid = await get_primary_engagement(
            self.dataloader.graphql_client, uuid
        )
        if primary_engagement_uuid is None:
            return {}

        if "Engagement" not in self.settings.conversion_mapping.ldap_to_mo:
            return {}

        await self.perform_export_checks(uuid, primary_engagement_uuid)

        fetched_engagement = await self.dataloader.load_mo_engagement(
            primary_engagement_uuid, current_objects_only=False
        )
        if fetched_engagement is None:
            logger.error("Unable to load mo engagement")
            raise RequeueMessage("Unable to load mo engagement")
        delete = get_delete_flag(jsonable_encoder(fetched_engagement))

        template_dict = ChainMap(
            {"mo_employee_engagement": fetched_engagement}, mo_object_dict
        )

        # Convert & Upload to LDAP
        ldap_object = await self.converter.to_ldap(template_dict, "Engagement", dn)
        return {"Engagement": (ldap_object, delete)}

    async def _find_best_dn(
        self, uuid: EmployeeUUID, dry_run: bool = False
    ) -> DN | None:
        dns = await self.dataloader.find_mo_employee_dn(uuid)
        # If we found DNs, we want to synchronize to the best of them
        if dns:
            logger.info("Found DNs for user", dns=dns, uuid=uuid)
            best_dn = await apply_discriminator(
                self.settings, self.ldap_connection, dns
            )
            # If no good LDAP account was found, we do not want to synchronize at all
            if best_dn:
                return best_dn
            logger.warning(
                "Aborting synchronization, as no good LDAP account was found",
                dns=dns,
                uuid=uuid,
            )
            return None

        # If dry-running we do not want to generate real DNs in LDAP
        if dry_run:
            return "CN=Dry run,DC=example,DC=com"

        # If we did not find DNs, we want to make one
        try:
            # This call actually writes in LDAP, so make sure that is okay
            # TODO: Restructure the code so it does not actually write
            await self.perform_export_checks(uuid, uuid)
            best_dn = await self.dataloader.make_mo_employee_dn(uuid)
        except DNNotFound as error:
            # If this occurs we were unable to generate a DN for the user
            logger.error("Unable to generate DN")
            raise RequeueMessage("Unable to generate DN") from error
        return best_dn

    @with_exitstack
    async def listen_to_changes_in_employees(
        self,
        uuid: EmployeeUUID,
        exit_stack: ExitStack,
        dry_run: bool = False,
    ) -> dict[str, tuple[LdapObject, bool]]:
        """Synchronize employee data from MO to LDAP.

        Args:
            uuid: UUID of the changed employee.
            exit_stack: The injected exit-stack.
        """
        exit_stack.enter_context(bound_contextvars(uuid=str(uuid)))
        logger.info("Registered change in an employee")

        best_dn = await self._find_best_dn(uuid, dry_run=dry_run)
        if best_dn is None:
            return {}

        exit_stack.enter_context(bound_contextvars(dn=best_dn))

        # Get MO employee
        changed_employee = await self.dataloader.moapi.load_mo_employee(
            uuid, current_objects_only=False
        )
        if changed_employee is None:
            logger.error("Unable to load mo employee")
            raise RequeueMessage("Unable to load mo object")
        logger.info("Found Employee in MO", changed_employee=changed_employee)

        mo_object_dict: dict[str, Any] = {"mo_employee": changed_employee}

        ldap_employee = await self.mo_person_to_ldap(uuid, best_dn, mo_object_dict)
        person_addresses = await self.mo_address_to_ldap(uuid, best_dn, mo_object_dict)
        org_unit_addresses = await self.mo_org_unit_address_to_ldap(
            uuid, best_dn, mo_object_dict
        )
        itusers = await self.mo_ituser_to_ldap(uuid, best_dn, mo_object_dict)
        engagements = await self.mo_engagement_to_ldap(uuid, best_dn, mo_object_dict)

        changes = {**person_addresses, **org_unit_addresses, **itusers, **engagements}
        all_changes = {"Employee": (ldap_employee, False), **changes}

        # If dry-running we do not want to makes changes in LDAP
        if dry_run:
            return all_changes

        # Upload person to LDAP
        ldap_employee = await self.move_ldap_object(ldap_employee, best_dn)
        await self.dataloader.modify_ldap_object(
            ldap_employee,
            "Employee",
            # We do not generally terminate people in MO
            delete=False,
        )

        # Upload changes
        for json_key, (ldap_object, delete) in changes.items():
            ldap_object = await self.move_ldap_object(ldap_object, best_dn)
            await self.dataloader.modify_ldap_object(
                ldap_object, json_key, delete=delete
            )

        return all_changes

    async def format_converted_objects(
        self,
        converted_objects: Sequence[MOBase],
        json_key: str,
    ) -> list[tuple[MOBase, Verb]]:
        """
        for Address and Engagement objects:
            Loops through the objects, and sets the uuid if an existing matching object
            is found
        for ITUser objects:
            Loops through the objects and removes it if an existing matchin object is
            found
        for all other objects:
            returns the input list of converted_objects
        """
        # We can't infer the type from json_key because of Terminate objects
        mo_class = one({type(o) for o in converted_objects})
        objects_in_mo: Sequence[MOBase]

        # Load addresses already in MO
        if issubclass(mo_class, Address):
            converted_objects = cast(Sequence[Address], converted_objects)

            assert all_equal([obj.person for obj in converted_objects])
            person = first(converted_objects).person

            assert all_equal([obj.org_unit for obj in converted_objects])
            org_unit = first(converted_objects).org_unit

            assert all_equal([obj.address_type for obj in converted_objects])
            address_type = first(converted_objects).address_type

            if person:
                objects_in_mo = await self.dataloader.load_mo_employee_addresses(
                    person.uuid,
                    address_type.uuid,
                )
            elif org_unit:
                objects_in_mo = await self.dataloader.load_mo_org_unit_addresses(
                    OrgUnitUUID(org_unit.uuid),
                    address_type.uuid,
                )
            else:
                logger.info(
                    "Could not format converted "
                    "objects: An address needs to have either a person uuid "
                    "OR an org unit uuid"
                )
                return []

            # TODO: It seems weird to match addresses by value, as value is likely to
            #       change quite often. Omada simply deletes and recreates addresses.
            #       Maybe we should consider doing the same here?
            value_key = "value"

        # Load engagements already in MO
        elif issubclass(mo_class, Engagement):
            converted_objects = cast(Sequence[Engagement], converted_objects)

            assert all_equal([obj.person for obj in converted_objects])
            person = first(converted_objects).person

            objects_in_mo = await self.dataloader.load_mo_employee_engagements(
                person.uuid
            )
            value_key = "user_key"
            user_keys = [o.user_key for o in objects_in_mo]

            # If we have duplicate user_keys, remove those which are the same as the
            # primary engagement's user_key
            if len(set(user_keys)) < len(user_keys):
                primaries = await self.dataloader.is_primaries(
                    [o.uuid for o in objects_in_mo]
                )
                num_primaries = quantify(primaries)
                if num_primaries > 1:
                    raise RequeueMessage(
                        "Waiting for multiple primary engagements to be resolved"
                    )
                # TODO: if num_primaries == 0, we cannot remove duplicates, is this a problem?

                if num_primaries == 1:
                    primary_engagement = objects_in_mo[primaries.index(True)]
                    logger.info(
                        "Found primary engagement",
                        uuid=str(primary_engagement.uuid),
                        user_key=primary_engagement.user_key,
                    )
                    logger.info("Removing engagements with identical user keys")
                    objects_in_mo = [
                        o
                        for o in objects_in_mo
                        # Keep the primary engagement itself
                        if o == primary_engagement
                        # But remove duplicate user-key engagements
                        or o.user_key != primary_engagement.user_key
                    ]

        elif issubclass(mo_class, ITUser):
            converted_objects = cast(Sequence[ITUser], converted_objects)

            assert all_equal([obj.person for obj in converted_objects])
            person = first(converted_objects).person
            assert person is not None

            assert all_equal([obj.itsystem for obj in converted_objects])
            itsystem = first(converted_objects).itsystem

            objects_in_mo = await self.dataloader.load_mo_employee_it_users(
                person.uuid, itsystem.uuid
            )

            value_key = "user_key"
        elif issubclass(mo_class, Employee):
            converted_objects = cast(Sequence[Employee], converted_objects)

            return [
                (converted_object, Verb.CREATE)
                for converted_object in converted_objects
            ]
        else:  # pragma: no cover
            raise AssertionError(f"Unknown mo_class: {mo_class}")

        mapper_template = self.converter.mapping["ldap_to_mo"][json_key].get(
            "_mapper_", None
        )
        if mapper_template is not None:
            mo_values_task = asyncio.gather(
                *[mapper_template.render_async({"obj": obj}) for obj in objects_in_mo]
            )
            ldap_values_task = asyncio.gather(
                *[
                    mapper_template.render_async({"obj": obj})
                    for obj in converted_objects
                ]
            )
            mo_values, ldap_values = await asyncio.gather(
                mo_values_task, ldap_values_task
            )
            mo_mapper = dict(zip(objects_in_mo, mo_values, strict=False))
            ldap_mapper = dict(zip(converted_objects, ldap_values, strict=False))
        else:
            # TODO: Refactor so this is handled using default templates instead
            mo_mapper = {obj: getattr(obj, value_key) for obj in objects_in_mo}
            ldap_mapper = {obj: getattr(obj, value_key) for obj in converted_objects}

        # Construct a map from value-key to list of matching objects
        values_in_mo = bucketdict(objects_in_mo, mo_mapper.get)
        values_converted = bucketdict(converted_objects, ldap_mapper.get)

        # Only values in MO targeted by our converted values are relevant
        values_in_mo = {
            key: value for key, value in values_in_mo.items() if key in values_converted
        }

        # We need a mapping between MO objects and converted objects.
        # Without a mapping we cannot maintain temporality of objects in MO.

        # If we have more than one MO object for each converted, the match is ambigious.
        # Ambigious matches mean no mapping and must be handled by human intervention.
        ambigious_exception = RequeueMessage("Bad mapping: Multiple MO objects")
        value_in_mo = {
            key: only(value, too_long=ambigious_exception)
            for key, value in values_in_mo.items()
        }

        # If we have more than one converted per value-key there cannot be a mapping.
        # This probably means we have a misconfiguration of the integration.
        # Perhaps a bad discriminator between MO objects per converted object.
        bad_discriminator_exception = RequeueMessage("Bad mapping: Multiple converted")
        value_converted = {
            key: one(value, too_long=bad_discriminator_exception)
            for key, value in values_converted.items()
        }

        # At this point we know a mapping exists from converted objects to MO objects
        mapping = [
            (value_converted[key], value_in_mo.get(key)) for key in value_converted
        ]

        # Partition the mapping into creates and updates
        creates, updates = partition(
            # We have an update if there is no MO object in the mapping
            star(lambda converted, mo_object: mo_object is not None),
            mapping,
        )
        updates = cast(Iterator[tuple[MOBase, MOBase]], updates)

        # Convert creates to operations
        operations = [
            (converted_object, Verb.CREATE) for converted_object, _ in creates
        ]

        # Convert updates to operations
        mo_attributes = set(self.converter.get_mo_attributes(json_key))
        for converted_object, matching_object in updates:
            # Convert our objects to dicts
            mo_object_dict_to_upload = matching_object.dict()
            # Need to by_alias=True to extract the terminate_ field as its alias,
            # _terminate_. Only the *intersection* of attribute names from
            # mo_object_dict_to_upload and converted_mo_object_dict are used.
            converted_mo_object_dict = converted_object.dict(by_alias=True)

            # Update the existing MO object with the converted values
            # NOTE: UUID cannot be updated as it is used to decide what we update
            # NOTE: objectClass is removed as it is an LDAP implemenation detail
            # TODO: Why do we not update validity???
            mo_attributes = mo_attributes - {"validity", "uuid", "objectClass"}
            # Only copy over keys that exist in both sets
            mo_attributes = mo_attributes & converted_mo_object_dict.keys()

            update_values = {
                key: converted_mo_object_dict[key] for key in mo_attributes
            }
            logger.info(
                "Setting values on upload dict",
                uuid=mo_object_dict_to_upload["uuid"],
                values=update_values,
            )

            mo_object_dict_to_upload.update(update_values)
            converted_object_uuid_checked = mo_class(**mo_object_dict_to_upload)

            # TODO: Try to get this reactivated, see: 87683a2b
            # # If an object is identical to the one already there, it does not need
            # # to be uploaded.
            # if converted_object_uuid_checked == matching_object:
            #     logger.info(
            #         "Converted object is identical "
            #         "to existing object, skipping"
            #     )
            #     continue
            # We found a match, so we are editing the object we matched
            operations.append((converted_object_uuid_checked, Verb.EDIT))

        return operations

    @wait_for_import_to_finish
    @with_exitstack
    async def import_single_user(
        self, dn: DN, exit_stack: ExitStack, manual_import: bool = False
    ) -> None:
        """Imports a single user from LDAP into MO.

        Args:
            dn: The DN that triggered our event changed in LDAP.
            manual_import: Whether this import operation was manually triggered.
        """
        exit_stack.enter_context(bound_contextvars(dn=dn, manual_import=manual_import))

        logger.info("Importing user")

        # Get the employee's UUID (if they exists)
        employee_uuid = await self.dataloader.find_mo_employee_uuid(dn)
        if employee_uuid:
            # If we found an employee UUID, we want to use that to find all DNs
            dns = await self.dataloader.find_mo_employee_dn(employee_uuid)
        else:  # We did not find an employee UUID
            # Check if we wish to create the employee or not
            create_employee = self.settings.conversion_mapping.ldap_to_mo[
                "Employee"
            ].import_to_mo_as_bool(manual_import)
            if not create_employee:
                # If we do not want to create the employee and it does not exist, there
                # is no more to be done, as we cannot create dependent resources with no
                # employee to attach them to.
                logger.info("Employee not found in MO, and not configured to create it")
                return
            logger.info("Employee not found, but configured to create it")

            # As we wish to create an employee, we need to generate an UUID for it
            employee_uuid = uuid4()
            logger.info(
                "Employee not found in MO, generated UUID", employee_uuid=employee_uuid
            )
            # At this point employee_uuid is always set

            # We want to create our employee using the best possible LDAP account.
            # By default, we will use the account that was provided to us in the event.
            dns = {dn}

            # However we may be able to find other accounts using the CPR number on the
            # event triggered account, by searching for the CPR number in all of LDAP.
            # Note however, that this will only succeed if there is a CPR number field.
            if self.settings.ldap_cpr_attribute:
                ldap_obj = await get_ldap_object(
                    self.ldap_connection,
                    dn,
                    attributes=[self.settings.ldap_cpr_attribute],
                )
                cpr_no = getattr(ldap_obj, self.settings.ldap_cpr_attribute)
                # Only attempt to load accounts if we have a CPR number to do so with
                # and only if the CPR number is not the commonly used test CPR number
                if cpr_no and cpr_no != "0000000000":
                    ldap_objs = await self.dataloader.load_ldap_cpr_object(
                        cpr_no, "Employee"
                    )
                    dns = {obj.dn for obj in ldap_objs}

        # At this point 'employee_uuid' is an UUID that may or may not be in MO
        # At this point 'dns' is a list of LDAP account DNs

        # We always want to synchronize from the best LDAP account, instead of just
        # synchronizing from the last LDAP account that has been touched.
        # Thus we process the list of DNs found for the user to pick the best one.
        best_dn = await apply_discriminator(self.settings, self.ldap_connection, dns)
        # If no good LDAP account was found, we do not want to synchronize at all
        if best_dn is None:
            logger.info(
                "Aborting synchronization, as no good LDAP account was found",
                dns=dns,
                employee_uuid=employee_uuid,
            )
            return

        # At this point, we have the best possible DN for the user, and their employee UUID
        if dn != best_dn:
            logger.info(
                "Found better DN for employee",
                best_dn=best_dn,
                dns=dns,
                employee_uuid=employee_uuid,
            )
        dn = best_dn
        exit_stack.enter_context(bound_contextvars(dn=dn))

        # First import the Employee, then Engagement if present, then the rest.
        # We want this order so dependencies exist before their dependent objects
        # TODO: Maybe there should be a dependency graph in the future
        detected_json_keys = set(self.settings.conversion_mapping.ldap_to_mo.keys())
        # We always want Employee in our json_keys
        detected_json_keys.add("Employee")
        priority_map = {"Employee": 1, "Engagement": 2}
        json_keys = sorted(detected_json_keys, key=lambda x: priority_map.get(x, 3))

        json_keys = [
            json_key
            for json_key in json_keys
            if self.settings.conversion_mapping.ldap_to_mo[
                json_key
            ].import_to_mo_as_bool(manual_import)
        ]
        logger.info("Import to MO filtered", json_keys=json_keys)

        json_keys = [
            json_key
            for json_key in json_keys
            if await self.perform_import_checks(dn, json_key)
        ]
        logger.info("Import checks executed", json_keys=json_keys)

        await asyncio.gather(
            *[
                self.import_single_user_entity(json_key, dn, employee_uuid)
                for json_key in json_keys
            ]
        )

    async def import_single_user_entity(
        self, json_key: str, dn: str, employee_uuid: UUID
    ) -> None:
        logger.info("Loading object", dn=dn, json_key=json_key)
        loaded_object = await get_ldap_object(
            self.ldap_connection, dn, self.converter.get_ldap_attributes(json_key)
        )
        logger.info(
            "Loaded object",
            dn=dn,
            json_key=json_key,
            loaded_object=loaded_object,
        )

        converted_objects = await self.converter.from_ldap(
            loaded_object,
            json_key,
            employee_uuid=employee_uuid,
        )
        if not converted_objects:
            logger.info("No converted objects", dn=dn)
            return

        logger.info(
            "Converted 'n' objects ",
            n=len(converted_objects),
            dn=dn,
        )
        if json_key == "Custom":
            await asyncio.gather(
                *[
                    obj.sync_to_mo(self.dataloader.graphql_client)
                    for obj in converted_objects
                ]
            )
            return

        converted_objects = await self.format_converted_objects(
            converted_objects, json_key
        )
        if not converted_objects:  # pragma: no cover
            logger.info("No converted objects after formatting", dn=dn)
            return

        logger.info(
            "Importing objects",
            converted_objects=converted_objects,
            dn=dn,
        )
        await self.dataloader.create_or_edit_mo_objects(converted_objects)
